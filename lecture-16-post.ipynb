{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZewGRl_gPWJM"
   },
   "source": [
    "# Lecture 16: Line search\n",
    "\n",
    "Last time we saw that optimization algorithms can sometimes fail pathologically. [Line search](https://en.wikipedia.org/wiki/Line_search) methods help to alleviate this problem. Here, we'll practice implementing the [Wolfe conditions](https://en.wikipedia.org/wiki/Wolfe_conditions), which are a set of inequalities that any optimization step we make should satisfy. \n",
    "\n",
    "Let's call the function we're trying to minimize $f(\\underline{x})$, and let $\\underline{s}$ be the step direction of a proposed update. Let $t$ be the distance that we step in the $\\underline{s}$ direction. The Wolfe conditions then require **sufficient decrease** of the function,\n",
    "\n",
    "$$\n",
    "f(\\underline{x} + t \\underline{s}) \\leq f(\\underline{x}) + \\alpha t \\nabla f(\\underline{x})^T\\underline{s}\\,,\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is a parameter that we choose (typically around $10^{-3}$). They also require that the slope of the function decreases significantly, which is captured by a **curvature condition**\n",
    "\n",
    "$$\n",
    "\\nabla f(\\underline{x}+t\\underline{s})^T \\underline{s} \\geq \\gamma \\nabla f(\\underline{x})^T \\underline{s}\\,.\n",
    "$$\n",
    "\n",
    "Here again $\\gamma$ is a parameter that we must choose. Naturally, $\\gamma\\in(\\alpha,1)$, and typically $\\gamma$ is taken to be between 0.1 and 0.9.\n",
    "\n",
    "\n",
    "### Example: Correcting steepest descent optimization of a quadratic function\n",
    "\n",
    "Let's return to the previous example of minimizing a quadratic function with steepest descent, but this time we will apply the Wolfe conditions and a [backtracking line search](https://en.wikipedia.org/wiki/Backtracking_line_search). \n",
    "\n",
    "We will consider a quadratic function of a single variable\n",
    "\n",
    "$$\n",
    "f(x) = a x^2 + bx + c\\,,\n",
    "$$\n",
    "\n",
    "with $a>0$. First, let's visualize the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "cIZgEmmsPWJN",
    "outputId": "ba71b8c8-864c-4b4e-e6a9-03c62691f739"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xd41eX9//HnO4sAgTASAgQIK+xN\nWApFGS7co4qiVK22tq5qv/3a6re7v9raZYdVERQX7q114WCPgOw9kjACCSsEQvb9+yMxF1JGAsm5\nz8l5Pa4rFzmHEz8vEfPKfT73MOccIiIiABG+A4iISPBQKYiISBWVgoiIVFEpiIhIFZWCiIhUUSmI\niEgVlYKIiFRRKYiISBWVgoiIVInyHaCmEhISXMeOHX3HEBEJKUuWLNnjnEs81etCrhQ6duxIenq6\n7xgiIiHFzDKr8zq9fSQiIlVUCiIiUkWlICIiVVQKIiJSRaUgIiJVVAoiIlJFpSAiIlXCphRmb8zl\nsS82+Y4hIhLUwqYU5mzcw18+3kBOfqHvKCIiNVJYUsZfPl7PzgNH6vxaYVMK3x7SntJyx2tLtvuO\nIiJSIx+u2sXfP9tExp7DdX6tsCmFLolxDO3UgpcXb8M55zuOiEi1zViURUrLRgzv3LLOrxU2pQAw\ncWh7MvcWMH/LXt9RRESqZUvuIRZu3ce1Q9oTEWF1fr2wKoUL+7ShaWwULy3a5juKiEi1vJy+jcgI\n4+pB7QJyvbAqhdjoSK4YmMyHq3ax/3Cx7zgiIidVXFrO60u2M7ZHK1o1jQ3INcOqFACuG9qB4rJy\n3vxqh+8oIiInNXPtbvYcKmbi0A4Bu2bYlULPNk3p3y6elxZn6YaziAS1lxZvo018LN/qdsqzcWpN\n2JUCVIwWNuw+xNKsA76jiIgc1/b9BczamMs1ae2JDMAN5q+FZSlc0r8tjWIieXlxlu8oIiLH9Up6\nxZqqb6cF5gbz18KyFOIaRHFJv7a8uzyb/MIS33FERL6hrNzxavo2RqUm0q55o4BeOyxLAeC6oe05\nUlLGO8t3+o4iIvINszbkkp1XyMQh7QN+7bAthQHtm9GjdROtWRCRoDNjURYJcTGM7ZkU8GuHbSmY\nGdcOac/KHXms2pHnO46ICAA5+YXMXJfDVYPaERMV+G/RYVsKAFcMTCYmKoKXF2u0ICLB4bUl2ykr\nd1zr4a0jCPNSaNYohov6tOatr3ZQUFzqO46IhLmycseLC7MY3rkFnRPjvGQI61IAmDi0A/lFpbyr\nG84i4tmsjbls33+EScNTvGUI+1IY2qkF3ZLieH6B1iyIiF8vLMgkIa4B5/Vq7S1DQErBzNqb2edm\ntsbMVpvZPZXP/9LMdpjZssqPiwKR55hs3DAshZU78li+TSucRcSPHQeO8Nm6HK4d4ucG89cCdeVS\n4H7nXC9gOPBDM+tV+Xt/dc4NqPz4IEB5vuGKQck0jI7k+QWZPi4vIsJLi7JwENDN744nIKXgnMt2\nzi2t/DwfWAskB+La1dE0NprLB7bl3RU7ySvQCmcRCaySsnJeWryNc7u3CvgK5mMFfIxiZh2BgcDC\nyqfuNLMVZjbNzJqf4GtuN7N0M0vPzc2tk1w3DEuhsKSc15bqDGcRCaxP1uwmN7+IScP9jhIgwKVg\nZnHA68C9zrmDwL+BLsAAIBv48/G+zjn3pHMuzTmXlphYN1vI9kmOZ2CHZrywMFNbaotIQD2/IJPk\nZg0Z3a2V7yiBKwUzi6aiEF5wzr0B4Jzb7Zwrc86VA1OAoYHKczyThqWwJfcw8zfrDGcRCYzNuYeY\nt3kv1w/rENAtsk8kULOPDJgKrHXO/eWo59sc9bIrgFWByHMiE/q1oVmjaJ5fqBvOIhIYLy7MIirC\nuCbAW2SfSFSArnM2cCOw0syWVT73M2CimQ0AHJABfC9AeY4rNjqSawa34+m5GeQcLAzYmagiEp4K\nS8p4bcl2zu/TmlZNguP7TUBKwTk3BzjeuMjLFNSTuX5YClNmb+Xlxdu4a2yq7zgiUo+9tyKbvCMl\n3DDM/w3mr4X9iuZjdUpozKjUBGYsyqKsXDecRaTuPL8gk86JjRnRuaXvKFVUCsdxw7AUduYV8tm6\nHN9RRKSeWrUjj2XbDnDDsBQqbrsGB5XCcYzr2YrWTWO1wllE6sz0eRk0jI7k6sHBcYP5ayqF44iK\njOC6oe2ZtTGXzL2HfccRkXpm3+Fi3l6+kysGJRPfMNp3nG9QKZzAxKEdiDTj2fkaLYhI7Xp58TaK\nS8uZPKKj7yj/RaVwAklNY7mobxteWbyNw0U6gEdEakdpWTnPL8hkROeWdG/dxHec/6JSOInJZ3Uk\nv6iUN77a4TuKiNQTn67NYceBI0w+q6PvKMelUjiJQR2a0a9dPNPnZWg/JBGpFdPnZdA2PpZxPf3v\nc3Q8KoWTMDMmj+jIppxDzN2k/ZBE5Mys35XP/C17mTQihajI4Pz2G5ypgsjF/duQEBfDM/O2+o4i\nIiHu2fkZxERFcN2Q4FnBfCyVwik0iIpk4tAOzFyXQ9beAt9xRCRE5R0p4Y2lO7isf1taNI7xHeeE\nVArVcMOwlMrpqRm+o4hIiHo1fRtHSsqC9gbz11QK1dA6PpYL+rTmlfRtFBRreqqI1Ex5ueO5BZmk\npTSnT3K87zgnpVKopu+c1ZGDhaW8qempIlJDX2zIIXNvQdCPEkClUG2DU5rTJ7mppqeKSI09My+T\nVk0acEGf1r6jnJJKoZq+np66YfchHdcpItW2KSefWRtymTQ8heggnYZ6tOBPGEQuqZw1MG1uhu8o\nIhIips7JoEFURFAdpHMyKoUaiI2O5PqhHZi5bjcZe7R7qoic3L7DxbyxdDtXDkqmZVwD33GqRaVQ\nQzedlUJ0RARPz9ViNhE5uRcWZFJUWs4tZ3fyHaXaVAo11KpJLJcOaMsr6ds5UFDsO46IBKmi0jKm\nz8/knO6JpCYF326oJ6JSOA23juzEkZIyXlyU5TuKiASpd5dns+dQEbeODJ1RAqgUTkvPNk0Z2TWB\n6fMyKC4t9x1HRIKMc46nZm+he1ITRnZN8B2nRlQKp+m7ozqx+2AR76/c6TuKiASZ+Zv3sm5XPreO\n7ISZ+Y5TIyqF0zS6WyKpreJ4avZWLWYTkW94as5WEuJiuHRAW99RakylcJrMjFtHdmL1zoPM36LF\nbCJSYXPuIT5bl8Ok4SnERkf6jlNjKoUzcPnAZFo2jmHqbE1PFZEK0+ZsJSYqgknDU3xHOS0BKQUz\na29mn5vZGjNbbWb3VD7fwsw+MbONlb82D0Se2hIbHcmNI1KYuS6HzbmHfMcREc/2Hy7m9aXbuWJA\nMgkhsljtWIEaKZQC9zvnegHDgR+aWS/gAWCmcy4VmFn5OKRMGp5CTFQE0+ZotCAS7l5clEVhSTm3\njgqtaahHC0gpOOeynXNLKz/PB9YCycBlwPTKl00HLg9EntqUENeAKwcm8/rS7ew7rMVsIuGqsKSM\np+dm8K1uiXQLocVqxwr4PQUz6wgMBBYCSc657Mrf2gUkneBrbjezdDNLz83NDUjOmrh1ZCcKS8p5\nfkGm7ygi4snrS7ez51AR3x/d2XeUMxLQUjCzOOB14F7n3MGjf89VzOs87txO59yTzrk051xaYmJi\nAJLWTGpSE8b0aMUz8zI4UlzmO46IBFhZuWPKrC30bxfPiM4tfcc5IwErBTOLpqIQXnDOvVH59G4z\na1P5+22AnEDlqW3fH92FfYeLeXXJNt9RRCTAPlq9i4y9BXxvdJeQW6x2rEDNPjJgKrDWOfeXo37r\nHWBy5eeTgbcDkacuDOnYnMEpzXly1hZKy7T1hUi4cM7xxJeb6diyEef3Dv6T1U4lUCOFs4EbgTFm\ntqzy4yLgYWC8mW0ExlU+DklmxvdHd2H7/iO8vzL71F8gIvXC/C17Wb49j9u+1ZnIiNAeJQBEBeIi\nzrk5wIn+tMYGIkMgjO3RitRWcfz7i81c2r9tyA8jReTUnvhyCwlxMVw1qJ3vKLVCK5prUUSE8b3R\nXVi3K58vNgTfLCkRqV1rdh7kyw253Hx2p5Dc0uJ4VAq17NL+bWkTH8vjX2z2HUVE6tgTszbTOCaS\nScNCc0uL41Ep1LKYqAi+O6ozC7fuY2nWft9xRKSObNtXwHsrspk4tAPxjaJ9x6k1KoU6cN2Q9sQ3\njNZoQaQemzpnKwYhvaXF8agU6kDjBlFMHpHCx2t2sykn33ccEall+w4X89LiLC4bkEyb+Ia+49Qq\nlUIdmXxWR2KjI3jiyy2+o4hILXtm7lYKS8r5XohvaXE8KoU60jKuAdemteetZTvIzjviO46I1JKD\nhSU8PS+DC3q3DumN705EpVCHbvtWZ5xDowWReuS5+ZnkF5Zy55iuvqPUCZVCHWrXvBFXDkpmxqIs\ncvILfccRkTNUUFzKU7O3cG73RPokx/uOUydUCnXsB+d0paSsnCmzNFoQCXUvLsxif0EJd45J9R2l\nzqgU6ljHhMZcNiCZ5xdksfdQke84InKaCkvKeGLWFs7q0pLBKSF1cnCNqBQC4IfndqWwtIypOrJT\nJGS9mr6N3Pyiensv4WsqhQDo2iqOi/q24dn5mRwo0JGdIqGmuLScx7/cQlpK85A/ROdUVAoBcteY\nrhwqKuXpuRm+o4hIDb311Q52HDjCnWO61vvdj1UKAdKjdVPO65XE03O3kl9Y4juOiFRTaVk5j32x\nib7J8YzuFnzHAdc2lUIA3TUmlYOFpTw7P9N3FBGppvdXZpOxtyAsRgmgUgiovu3iObd7Ik/N3sLh\nolLfcUTkFMrKHY/O3Ej3pCaM75nkO05AqBQC7K6xqewvKOGFhRotiAS7d5bvYEvuYX40PpWIenDU\nZnWoFAJsUIfmjEpN4MlZWygo1mhBJFiVlpXz6Kcb6dmmKef1au07TsCoFDy4d1wqew4V696CSBB7\na9lOMvYW8KNx4TNKAJWCF4NTWjC6WyJPfLmZQ7q3IBJ0SsrK+fvMjfRJbsr4XuFxL+FrKgVP7hvf\njf0FJTytVc4iQeeNpdvJ2lfAj8Z1C4sZR0dTKXjSv30zxvVMYsrsLeQd0boFkWBRXFrOPz7bRP92\n8Yzp0cp3nIBTKXh03/huHCwsZeps7aAqEixeW7Kd7fuPcO/48BslgErBq15tm3JR39ZMm5vB/sPa\nE0nEt6LSMv71+SYGdmjGOWGwevl4VAqe3TuuG4eLS3lC5y2IePdK+nZ2HDgSlvcSvhaQUjCzaWaW\nY2arjnrul2a2w8yWVX5cFIgswaZbUhMu7d+W6fMyyM3XeQsivhSWlPGvzzaRllKxlihcBWqk8Axw\nwXGe/6tzbkDlxwcByhJ07hmbSlFpGY9/udl3FJGwNX1eBrsOFvLj87uH7SgBAlQKzrlZwL5AXCsU\ndU6M48pB7XhuQSbZeUd8xxEJO3lHSnjsi82M7pbI8Hp+XsKp+L6ncKeZrah8e6n+nm9XDfeMTcU5\nx6OfbvQdRSTsTJlVMTX8f87v7juKdzUuBTNrbGaRtXDtfwNdgAFANvDnk1zzdjNLN7P03NzcWrh0\n8GnfohE3DEvhlfRtbMo55DuOSNjIyS9k6pytXNK/LX2S433H8e6UpWBmEWZ2vZm9b2Y5wDog28zW\nmNkjZnZaB5Y653Y758qcc+XAFGDoSV77pHMuzTmXlphYf6eJ3TWmK41iovjTR+t9RxEJG//8bBMl\nZeXcP76b7yhBoTojhc+p+In+p0Br51x751wrYCSwAPiDmU2q6YXNrM1RD68AVp3oteGiZVwDbhvV\nmQ9X72Jp1n7fcUTqvay9Bby4MItvD2lPx4TGvuMEheqUwjjn3G+ccysqf6oHwDm3zzn3unPuKuDl\nk/0DzGwGMB/obmbbzexW4I9mttLMVgDnAj86g3+PeuO7ozqREBfDH/6zDuec7zgi9dpfPllPZIRx\nz9hU31GCxilLwTlXAmBmj9oJ5ml9/ZqT/DMmOufaOOeinXPtnHNTnXM3Ouf6Ouf6Oecudc5ln96/\nQv3SuEEUd49NZeHWfXyxoX7ePxEJBmuzD/L28p3cfHYnkprG+o4TNGpyozkfeMfMGgOY2flmNrdu\nYoW364Z0oEOLRvzxw/WUl2u0IFIX/vTRepo0iOKO0V18Rwkq1S4F59xDwAzgi8oyuA94oK6ChbOY\nqAjuP68ba7MP8s7ynb7jiNQ7C7fsZea6HL43ugvxjaJ9xwkq1S4FMxsL3AYcBhKAu51zs+sqWLi7\npF9berdtyp8+Xk9RaZnvOCL1Rnm547fvr6VNfCy3nN3Jd5ygU5O3jx4E/s85dw5wNfCymY2pk1RC\nRITxvxf0YPv+Izy/IMt3HJF6453lO1m5I48fn9edhjG1seSqfqnJ20djnHNzKj9fCVwI/LauggmM\nSk1gVGoCf5+5kQMF2lpb5EwVlpTxxw/X0Se5KVcMTPYdJyhVZ/HaiWYcZQNjT/YaOTNmxoMTepJf\nWMKjM7X9hciZmjZ3KzvzCvnZRT2JiNC3reOpzkjhMzO7y8w6HP2kmcUAI8xsOjC5TtIJPVo35doh\nHXhufiZbcrX9hcjp2nOoiMc+38y4nq04q0v4bo19KtUphY1AGfCmme2s3N5iS+XzE4G/OeeeqcOM\nYe++8d1oEBXB7/+zzncUkZD16KcbOVJSxgMX9vQdJahVpxSGOOceAwzoQMVbRoOccynOuducc1/V\naUIhsUkDfnBuVz5Zs5t5m/f4jiMScjbl5PPioixuGNaBrq3ifMcJatUphZlmNh9IAm4C2gLa9D/A\nbh3ZieRmDfnte2sp04I2kRp5+D/raBQdqe0sqqE621z8GJhExVtInYD/A1aZ2WozO+meR1J7YqMj\n+d8Le7Am+yCvL93uO45IyJi3aQ+frs3hB+d2pWVcA99xgl5UdV7knNtsZuOccxu+fs7M4oA+dZZM\n/ssl/drw9Nyt/Omj9Uzo24bGDar1n08kbJWWlfOrd9fQrnlDbj67o+84IaEm6xQ2HPP4kHNuQe1H\nkhMxMx6a0Iuc/CKe0HnOIqf0wsIs1u/O56EJvYiN1kK16vB9HKfU0OCU5lzSvy2Pz9pC1t4C33FE\ngta+w8X8+eP1jOyawPm9k3zHCRkqhRD0s4t6EBVh/Ob9Nb6jiAStP3+8nsPFZfzikl5ofW31qRRC\nUJv4htw1JpVP1uzm8/U5vuOIBJ1VO/J4cVEWN41IITWpie84IUWlEKJuGdmRzgmN+fW7a7SLqshR\nnHP86t3VNG8Uw73jdO5yTakUQlSDqEh+cWlvtu45zNQ5W33HEQka7yzfyeKM/fzk/O7EN9RZCTWl\nUghho7slcl6vJP4xcxPZeVpPKFJQXMrvP1hH3+R4rklr7ztOSFIphLj/u7gX5c7xu/fX+o4i4t2j\nMzey62Ahv7y0F5HaBfW0qBRCXPsWjbjjnC68tyJb+yJJWFu/K5+ps7dybVp7Bqe08B0nZKkU6oHv\nj+5Cu+YN+cXbqykpK/cdRyTgyssdD721kiaxUTxwYQ/fcUKaSqEeiI2O5JeX9GZjziGmzN7iO45I\nwL22ZDuLM/bz04t60rxxjO84IU2lUE+M65XE+b2TePTTjVrpLGFl3+Fifv+ftQzp2JyrB7XzHSfk\nqRTqkV9d2ofoyAgeensVzml7bQkPD/9nLfmFpfz28r46YrMWqBTqkdbxsfz4vG7M2pDLuyuyfccR\nqXPpGft4JX07t47qRPfWWrlcGwJSCmY2zcxyzGzVUc+1MLNPzGxj5a/NA5GlvrtxREf6tYvn1++u\nIa+gxHcckTpTUlbOg2+uIrlZQx2eU4sCNVJ4BrjgmOceAGY651KBmZWP5QxFRhj/74q+7DtcxMMf\n6kxnqb+e+HIz63fn86tLe9MoRmeL1JaAlIJzbhaw75inLwOmV34+Hbg8EFnCQZ/keG45uxMzFmWR\nnnHsH7tI6NuUk8/fZ25iQr82jOulbbFrk897CknOua/f+N5FxRnQUkt+NL4byc0a8tM3VmrDPKlX\nyssd//v6Sho1qJiKLbUrKG40u4qpMiecLmNmt5tZupml5+bmBjBZ6GrcIIrfXt6HjTmH+Odnm3zH\nEak1zy3IZEnmfn5+cS8Sm+jM5drmsxR2m1kbgMpfT3gwgHPuSedcmnMuLTExMWABQ925PVpx5aBk\nHvtiM6t25PmOI3LGtu8v4A8frmN0t0SuGJjsO0695LMU3gEmV34+GXjbY5Z66+cX96JF4xj+57UV\nFJdqCwwJXc45fvbmKgz43RV9dJpaHQnUlNQZwHygu5ltN7NbgYeB8Wa2ERhX+VhqWbNGMfzu8j6s\nzT7Iv7/Y7DuOyGl7Y+kOZm3I5ScX9KBd80a+49RbAZnH5ZybeILfGhuI64e783q35tL+bfnn5xs5\nv08SPVo39R1JpEZ2Hyzk1++tIS2lOTcOT/Edp14LihvNUvd+eWlv4htG8z+vrqBUO6lKCHHO8b+v\nr6CwpIw/XN1PW1nUMZVCmGjROIZfX9aHlTvyeGKWdlKV0PHS4m18sT6Xn17Ygy6Jcb7j1HsqhTBy\nUd82TOjXhr99uoHVOzUbSYJf1t4CfvPeGs7q0pKbRnT0HScsqBTCzG8v60PzRjH86OVlFJZoUZsE\nr7Jyx49fXU6kGY9c019vGwWISiHMNG8cwyPX9GfD7kP88cP1vuOInNDUOVtYlLGPX1zam+RmDX3H\nCRsqhTA0ulsik0ekMG3uVuZs1LnOEnzW78rnTx9t4LxeSVw1SIvUAkmlEKYeuLAnXRIb8+NXl3Og\noNh3HJEqRaVl3PfKMprERvH/ruyrRWoBplIIUw1jIvnbtQPZc6iIh97SSW0SPB75cD2rdx7k4av6\nkRCnvY0CTaUQxvq2i+fecam8tyKbt5ft9B1HhM/X5/DUnK3cNCKF8doS2wuVQpj7/ugupKU058E3\nV7J1z2HfcSSM5eQX8uNXltOjdRN+dlFP33HClkohzEVFRvDoxIFERUZw14ylOntBvCgvd9z/ynIO\nF5fyj4kDiY2O9B0pbKkUhORmDfnTNf1ZteMgv/9AR3hK4E2ZvYXZG/fw84t7k5rUxHecsKZSEADG\n90rilrM78cy8DD5avct3HAkjy7cd4JGP1nNB79ZMHNred5ywp1KQKg9c2IN+7eL5n1eXs31/ge84\nEgbyCkq4c8ZSWjVpwMNXafppMFApSJWYqAj+MXEg5Q7unvEVJdpNVepQebnj/leXkX2gkH9cP5Bm\njWJ8RxJUCnKMlJaNefiqvizNOsAfP9T9Bak7j8/azKdrc3hwQk8Gp7TwHUcqqRTkv1zcry03jUhh\nyuytvLdC6xek9s3bvIc/fbSeCf3a8J2zOvqOI0dRKchxPTShF4NTmvOT11awYXe+7zhSj+zKK+Tu\nGV/RKaExf7iqn+4jBBmVghxXTFQEj90wiMYNovjec0s4WFjiO5LUAyVl5dz54lIKist4fNJg4hoE\n5ERgqQGVgpxQUtNY/nX9ILbtK+C+l5dTXq79keTM/PrdNaRn7uf3V/bVeoQgpVKQkxraqQUPTejJ\np2t388/PN/mOIyHshYWZPLcgk+99qzOXDdB22MFKpSCnNPmsjlwxMJm/frpBC9vktCzcspdfvL2a\nc7on8pMLeviOIyehUpBTMjN+f2Vf+rVrxo9eXqbznaVGtu0r4I4XltKhZSMevW4gkTpWM6ipFKRa\nYqMjmXLjYOIbRnPb9HRy8gt9R5IQcLiolNueTaekrJynbkojvmG070hyCioFqbZWTWOZclMa+wtK\nuP3ZJRSWaEdVObGycsd9ryxjw+58/nn9IDonxvmOJNWgUpAa6ZMcz9+uG8CybQf4yWsrdGKbnNDv\n3l/LR6t389CEXozulug7jlST91IwswwzW2lmy8ws3XceObXze7fmJxd0553lO/nrJxt8x5EgNG3O\nVqbN3crNZ3fklpGdfMeRGgiWlSPnOuf2+A4h1XfH6C5k7ing759tIik+lhuGpfiOJEHiw1W7+M37\nazi/dxIPTejlO47UULCUgoQYM+N3V/Qh91AR//fWKhLiGnB+79a+Y4lnX2Xt556XvqJ/u2b87VrN\nNApF3t8+AhzwsZktMbPbfYeR6ouKjOCf1w+kb7tm3D3jK9Iz9vmOJB5t3XOY705Pp3V8LFMnp9Ew\nRkdqhqJgKIWRzrlBwIXAD83sW8e+wMxuN7N0M0vPzc0NfEI5oUYxUUybnEbbZg25dXo6m3K0eV44\n2nngCJOeWgjA098ZQsu4Bp4TyenyXgrOuR2Vv+YAbwJDj/OaJ51zac65tMREzWIINi3jGjD95qFE\nR0Zw09RFOrUtzOw5VMSkqQs5eKSE6bcM1dTTEOe1FMyssZk1+fpz4Dxglc9Mcno6tGzE9FuGcKio\nlBueWsjug1rcFg4OFpYwedoidh44wrSbh9AnOd53JDlDvkcKScAcM1sOLALed8596DmTnKbebeOZ\nfstQ9uQXcf2UBew5VOQ7ktShI8Vl3PrMYjbszufxSYMZ0lGnp9UHXkvBObfFOde/8qO3c+53PvPI\nmRvYoTnTvjOEHQeOcOPURRwoKPYdSepAYUkZtz+XTnrmfv567QDO6d7KdySpJb5HClIPDevckik3\npbE55xCTpy3SAT31TGFJGbc9m86cTXv4w1X9uLhfW9+RpBapFKROjEpN5LEbBrF650FunLqIvAIV\nQ31wpLiM706vKIQ/XtWPb6e19x1JaplKQerMuF5JPD5pMGt3HmTilAXs1T2GkHakuIzvPruYuZv3\n8MjV/blGhVAvqRSkTo3rlcSUyWlszj3EdU8uIEezkkJSfmEJNz+ziHmb9/LI1f25enA735GkjqgU\npM6N7pbIMzcPZceBI1z75AJ2HjjiO5LUwN5DRVw/ZSGLM/bz128PUCHUcyoFCYgRXVry3K0V01Wv\neXw+m3IO+Y4k1bDzwBG+/cR8NuzO58kbB3P5QJ2tXN+pFCRgBqe0YMbtwykqLePqx+exJFN7JQWz\nzbmHuPrf88g5WMSztwxlbM8k35EkAFQKElB9kuN5446zadYwmuunLOTj1bt8R5LjWJyxj6v/PY+i\n0nJm3D6cYZ1b+o4kAaJSkIDr0LIRr99xFj3aNOX7zy/h+QWZviPJUd5etoMbpiykWaMYXr/jLG1d\nEWZUCuJFy7gGzLhtGOd0b8X7XaGDAAAK3UlEQVRDb63il++sprSs3HessOac4+8zN3LPS8sY0KEZ\nb9xxFh0TGvuOJQGmUhBvGsVEMeWmNL47shPPzMvgO08v1rYYnhSWlHH/K8v5yycbuHJgMs/dOpTm\njWN8xxIPVAriVWSE8dDFvXjk6n4s2rqPy/81V2cyBNi2fQVc9e95vPHVDu4b340/f7s/DaJ0QE64\nUilIULgmrT0zbh/GoaJSLv/XPN5bsdN3pLAwe2Mul/xzDln7Cpg6OY27x6ZipiM0w5lKQYLG4JQW\nvHPnSLolxXHni1/x87dXUVRa5jtWvVRe7vjX55uYPG0RSU1ieffOkZpyKgBE+Q4gcrS2zRry8vdG\n8McP1zFl9laWZu3nX9cPIqWlbnjWll15hdz3yjLmbd7LJf3b8oer+tIoRt8KpIJGChJ0oiMjeHBC\nL6bclEbW3gIu/vscXluyHeec72gh7+PVu7jw0Vl8lXWAP1zVl79fN0CFIN+gUpCgNb5XEu/fPYqe\nbZry41eXc/tzS3Sa22k6VFTKg2+u5PbnltC2WUPeu3sk1w7poPsH8l9UChLU2rdoxIzbh/PgRT35\nckMu5/11Fh+uyvYdK6R8uSGX8/86ixcXZXHbqE688YOz6JIY5zuWBCmVggS9yAjjtm915r27RtK2\nWSzff34ptz+bzg7ttnpSBwqKuf+V5UyetojY6Ahe+/4IHpzQS9NN5aQs1N6nTUtLc+np6b5jiCcl\nZeU8NXsrj87cQIQZ945L5eazOxEdqZ9vvlZe7nht6Xb++OE69heUcMfoLtw5piux0SqDcGZmS5xz\naad8nUpBQtG2fQX88p3VzFyXQ4/WTXhwQk9GpSb6juXdksz9/Ord1azYnsegDs34zeV96N1WexeJ\nSkHCgHOOj9fs5tfvrmHHgSOMSk3gpxf2pFfbpr6jBdy2fQX8+eP1vLVsJ0lNG/DAhT24fECybiRL\nFZWChI2i0jKem5/JPz7bxMHCEq4YmMzdY1LDYjO37Lwj/OOzTbyyeBuREcZ3R3XiB+d0pXEDTTOV\nb1IpSNjJKyjhsS838fTcDErLyrmkf1t+cE5Xurdu4jtarcvaW8BTc7bw0uJtOOeYOLQDPzy3K0lN\nY31HkyClUpCwlZNfyNTZW3l+QSaHi8sY17MVk8/qyMiuCSH/dsqybQeYMmsL/1mVTWSEceXAdtw1\ntivtmjfyHU2CnEpBwt6BgmKenpvB8wsy2Xu4mM4JjZk0PIWrBrUjvlG073jVll9YwjvLd/Ly4m2s\n2J5Hk9goJg1P4TtnddTIQKotZErBzC4AHgUigaeccw+f7PUqBampotIyPliZzbPzM/kq6wAxkRGc\n0z2RywcmM6ZHq6CcqllcWs7czXt4b3k2H6zM5khJGT1aN+HaIe25Jq09cbpnIDUUEqVgZpHABmA8\nsB1YDEx0zq050deoFORMrNqRx5tf7eCd5TvJzS8irkEU3+qWwJgeSZzTPZGEuAbesuUVlDBv8x4+\nWbubT9bsJr+wlCYNori4f1uuG9Kefu3iQ/7tL/GnuqXg+8eNocAm59wWADN7CbgMOGEpiJyJPsnx\n9EmO52cX9WT+5r28t2Inn63L4YOVuzCD3m2bkpbSgiEdW5DWsXmdvj2Tk1/Iyu15LMncz9xNe1i5\nI49yB01jozi/d2sm9G3DWV1bagWyBJTvUkgGth31eDswzFMWCSOREcbI1ARGpibgnGP1zoN8ti6H\neZv38NLiLJ6ZlwFAQlwDuiXF0S2pCalJcbSNb0jr+FjaxMcS3zD6pD+5l5c78o6UsK+gmOwDhWzd\ne5jMPYfZuucwq3ceZNfBwqosA9s3464xqYxKTaB/+2ZaoS3e+C6FajGz24HbATp06OA5jdQ3ZlY1\ngrh7bColZeWs3nmQJZn7WZd9kA2783klfRsFxd888CfCoHFMFA1jImkUU/HTfGm5o7TMUVxWzoGC\nYsqPeXe2QVQEHVs2ZnjnFvRt14x+7eLp1aap1hVI0PD9N3EH0P6ox+0qn/sG59yTwJNQcU8hMNEk\nXEVHRjCgfTMGtG9W9Vx5uWPXwUKy8wrZlVfIroOF7DtcREFxGUeKyygoLsOs4qf+qAgjJiqC5o1i\naNG44qNVk1g6JTSmVZMGRETovoAEL9+lsBhINbNOVJTBdcD1fiOJ/LeICKNts4a0bdbQdxSROuW1\nFJxzpWZ2J/ARFVNSpznnVvvMJCISznyPFHDOfQB84DuHiIjokB0RETmKSkFERKqoFEREpIpKQURE\nqqgURESkikpBRESqeN86u6bMLBfI9J2jUgKwx3eIU1DGMxfs+UAZa0Ow54Mzy5jinEs81YtCrhSC\niZmlV2crWp+U8cwFez5QxtoQ7PkgMBn19pGIiFRRKYiISBWVwpl50neAalDGMxfs+UAZa0Ow54MA\nZNQ9BRERqaKRgoiIVFEpnCEz+42ZrTCzZWb2sZm19Z3pWGb2iJmtq8z5ppk1O/VXBY6ZXWNmq82s\n3MyCavaHmV1gZuvNbJOZPeA7z7HMbJqZ5ZjZKt9ZjsfM2pvZ52a2pvK/8T2+Mx3LzGLNbJGZLa/M\n+CvfmY7HzCLN7Csze68ur6NSOHOPOOf6OecGAO8BP/cd6Dg+Afo45/oBG4Cfes5zrFXAlcAs30GO\nZmaRwL+AC4FewEQz6+U31X95BrjAd4iTKAXud871AoYDPwzCP8MiYIxzrj8wALjAzIZ7znQ89wBr\n6/oiKoUz5Jw7eNTDxkDQ3aRxzn3snCutfLiAimNPg4Zzbq1zbr3vHMcxFNjknNvinCsGXgIu85zp\nG5xzs4B9vnOciHMu2zm3tPLzfCq+qSX7TfVNrsKhyofRlR9B9f+xmbUDJgBP1fW1VAq1wMx+Z2bb\ngBsIzpHC0W4B/uM7RIhIBrYd9Xg7QfYNLZSYWUdgILDQb5L/VvnWzDIgB/jEORdsGf8G/AQor+sL\nqRSqwcw+NbNVx/m4DMA596Bzrj3wAnBnMGasfM2DVAznXwjGfFJ/mVkc8Dpw7zGj66DgnCurfAu4\nHTDUzPr4zvQ1M7sYyHHOLQnE9bwfxxkKnHPjqvnSF6g4WvQXdRjnuE6V0cy+A1wMjHUe5iHX4M8w\nmOwA2h/1uF3lc1IDZhZNRSG84Jx7w3eek3HOHTCzz6m4TxMsN+/PBi41s4uAWKCpmT3vnJtUFxfT\nSOEMmVnqUQ8vA9b5ynIiZnYBFUPPS51zBb7zhJDFQKqZdTKzGOA64B3PmUKKmRkwFVjrnPuL7zzH\nY2aJX8/IM7OGwHiC6P9j59xPnXPtnHMdqfg7+FldFQKoFGrDw5Vvg6wAzqNihkCw+SfQBPikcurs\n474DHc3MrjCz7cAI4H0z+8h3JoDKm/N3Ah9RcYP0Fefcar+pvsnMZgDzge5mtt3MbvWd6RhnAzcC\nYyr/7i2r/Ik3mLQBPq/8f3gxFfcU6nTaZzDTimYREamikYKIiFRRKYiISBWVgoiIVFEpiIhIFZWC\niIhUUSmIiEgVlYKIiFRRKYicocrzAsZXfv5bM/uH70wip0t7H4mcuV8AvzazVlTsAnqp5zwip00r\nmkVqgZl9CcQB51SeGyASkvT2kcgZMrO+VOyfU6xCkFCnUhA5A2bWhoot0y8DDlXuSCsSslQKIqfJ\nzBoBb1BxBvFa4Dd4OEtDpDbpnoKIiFTRSEFERKqoFEREpIpKQUREqqgURESkikpBRESqqBRERKSK\nSkFERKqoFEREpMr/B2IEShDN+TErAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define the quadratic function\n",
    "\n",
    "a =  2\n",
    "b = -1\n",
    "c =  0\n",
    "\n",
    "def f(x):\n",
    "    return a*x**2 + b*x + c\n",
    "\n",
    "\n",
    "# Plot the function\n",
    "\n",
    "x_values = np.arange(-3.5, 4, 0.01)\n",
    "y_values = f(x_values)\n",
    "\n",
    "sns.lineplot(x_values, y_values)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K9WyEeVuPWJT"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EKvqVGJ0PWJY"
   },
   "source": [
    "Like last time, we can compute the derivative \n",
    "\n",
    "$$\n",
    "\\nabla f(x) = 2ax + b\n",
    "$$\n",
    "\n",
    "and use this to set the steepest descent step direction. \n",
    "\n",
    "Above we chose $a=2$, $b=-1$, and $c=0$, so the minimum is located at $x^* = 1/4$.\n",
    "\n",
    "### Step 1. Compute the step direction\n",
    "\n",
    "In the code below we will compute the step direction for steepest descent. We'll start by defining a function for the derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "SUn-lL87PWJa",
    "outputId": "15251829-f5c6-4e57-e998-78e088430d93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-11\n"
     ]
    }
   ],
   "source": [
    "def df(x):\n",
    "    \"\"\" Returns the derivative of the quadratic function f(x) = a x^2 + b x + c\"\"\"\n",
    "    return 2*a*x + b\n",
    "\n",
    "\n",
    "# Choose the step direction\n",
    "\n",
    "x0 = 3       # Starting value for x\n",
    "s  = -df(x0) # FILL THIS IN\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HJtU2EqhPWJd"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HNAL7-dtPWJg"
   },
   "source": [
    "### Step 2. Choose the step length\n",
    "\n",
    "Last time, we simply set the step length $t=0.1$. This time we will compute it with the backtracking line search. \n",
    "\n",
    "As a reminder, this line search iterates through the following steps:\n",
    "\n",
    "1. Initialize $t=1$\n",
    "2. Check if **sufficient decrease** condition is satisfied\n",
    "    - If this check passes, continue\n",
    "    - If this check fails, set $t = \\beta_1 t$ and return to step 2\n",
    "    \n",
    "Here we use a parameter $\\beta_1<1$ to reduce the step size iteratively until sufficient decrease is satisfied. \n",
    "\n",
    "We'll make this algorithm a little more sophisticated by also including a check on the **curvature condition**. This might then be called a \"forward-backward\" line search:\n",
    "\n",
    "1. Initialize $t=1$\n",
    "2. Check if **sufficient decrease** condition is satisfied\n",
    "   - If this check passes, continue\n",
    "   - If this check fails, set $t = \\beta_1 t$ and return to step 2  \n",
    "3. Check if **curvature condition** is satisfied\n",
    "   - If this check passes, continue\n",
    "   - If this check fails, set $t = \\beta_2 t$ and return to step 2  \n",
    "    \n",
    "Here, we need to set another parameter $\\beta_2>1$. It's important that we also choose $\\beta_1, \\beta_2$ such that $\\beta_1 \\beta_2 \\neq 1$, otherwise we could get stuck in an infinite loop.\n",
    "\n",
    "In the example below, we'll choose $\\beta_1 = 0.4$, $\\beta_2 = 1.2$, $\\alpha = 10^{-3}$, and $\\gamma = 0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "lwZwgo5KPWJh",
    "outputId": "3f0e69f1-74af-4e1e-def6-f252616c9693"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSufficient decrease failed, reducing t 1.000000 --> 0.400000\n",
      "\tThe accepted step length is 0.400000\n"
     ]
    }
   ],
   "source": [
    "# Set line search parameters\n",
    "\n",
    "beta1 = 0.4    # Step size multiplier if sufficient decrease fails\n",
    "beta2 = 1.2    # Step size multiplier if curvature condition fails\n",
    "alpha = 0.001  # Sufficient decrease parameter\n",
    "gamma = 0.5    # Curvature condition parameter\n",
    "\n",
    "# Initialize step length and adjust until both checks passed\n",
    "\n",
    "x = x0\n",
    "t = 1 \n",
    "both_passed = False\n",
    "\n",
    "while not both_passed:\n",
    "    \n",
    "    # Check for sufficient decrease fail\n",
    "    \n",
    "    if f(x + (t*s)) > f(x) + (alpha * t * np.dot(df(x), s)):\n",
    "        print('\\tSufficient decrease failed, reducing t %lf --> %lf' % (t, beta1*t))\n",
    "        t = beta1 * t\n",
    "        \n",
    "    # If passed, check for curvature condition fail\n",
    "    \n",
    "    if np.dot(df(x + (t*s)), s) < gamma * np.dot(df(x), s):\n",
    "        print('\\tCurvature condition failed, increasing t %lf --> %lf' % (t, beta2*t))\n",
    "        t = beta2 * t\n",
    "        \n",
    "    # If both passed, exit the loop\n",
    "    \n",
    "    else:\n",
    "        both_passed = True\n",
    "        \n",
    "print('\\tThe accepted step length is %lf' % t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jDf5F30IPWJm"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "leitNsZuPWJs"
   },
   "source": [
    "### Step 3. Update the parameters\n",
    "\n",
    "The last step in the steepest descent algorithm is to update the parameters. We'll also check for convergence to the minimum by looking at the size of the derivative,\n",
    "\n",
    "$$\n",
    "\\left|\\nabla f(x)\\right| < \\epsilon\\,,\n",
    "$$\n",
    "\n",
    "for some suitably small value of $\\epsilon$.\n",
    "\n",
    "**Exercise**: Fill in the code below to find the minimum of $f(x)$ by steepest descent. We'll start at $x_0=3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1097
    },
    "colab_type": "code",
    "id": "RoDDONuqPWJu",
    "outputId": "55a5b86b-62ad-40af-c244-14e2544227c3",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter\tx\tf(x)\tdf/dx\n",
      "0\t3.0000\t15.0000\t11.0000\n",
      "\tSufficient decrease failed, reducing t 1.000000 --> 0.400000\n",
      "\tThe accepted step length is 0.400000\n",
      "1\t-1.4000\t5.3200\t-6.6000\n",
      "\tSufficient decrease failed, reducing t 1.000000 --> 0.400000\n",
      "\tThe accepted step length is 0.400000\n",
      "2\t1.2400\t1.8352\t3.9600\n",
      "\tSufficient decrease failed, reducing t 1.000000 --> 0.400000\n",
      "\tThe accepted step length is 0.400000\n",
      "3\t-0.3440\t0.5807\t-2.3760\n",
      "\tSufficient decrease failed, reducing t 1.000000 --> 0.400000\n",
      "\tThe accepted step length is 0.400000\n",
      "4\t0.6064\t0.1290\t1.4256\n",
      "\tSufficient decrease failed, reducing t 1.000000 --> 0.400000\n",
      "\tThe accepted step length is 0.400000\n",
      "5\t0.0362\t-0.0335\t-0.8554\n",
      "\tSufficient decrease failed, reducing t 1.000000 --> 0.400000\n",
      "\tThe accepted step length is 0.400000\n",
      "6\t0.3783\t-0.0921\t0.5132\n",
      "\tSufficient decrease failed, reducing t 1.000000 --> 0.400000\n",
      "\tThe accepted step length is 0.400000\n",
      "7\t0.1730\t-0.1131\t-0.3079\n",
      "\tSufficient decrease failed, reducing t 1.000000 --> 0.400000\n",
      "\tThe accepted step length is 0.400000\n",
      "8\t0.2962\t-0.1207\t0.1848\n",
      "\tSufficient decrease failed, reducing t 1.000000 --> 0.400000\n",
      "\tThe accepted step length is 0.400000\n",
      "9\t0.2223\t-0.1235\t-0.1109\n",
      "\tSufficient decrease failed, reducing t 1.000000 --> 0.400000\n",
      "\tThe accepted step length is 0.400000\n",
      "10\t0.2666\t-0.1244\t0.0665\n",
      "\tSufficient decrease failed, reducing t 1.000000 --> 0.400000\n",
      "\tThe accepted step length is 0.400000\n",
      "11\t0.2400\t-0.1248\t-0.0399\n",
      "\tSufficient decrease failed, reducing t 1.000000 --> 0.400000\n",
      "\tThe accepted step length is 0.400000\n",
      "12\t0.2560\t-0.1249\t0.0239\n",
      "\tSufficient decrease failed, reducing t 1.000000 --> 0.400000\n",
      "\tThe accepted step length is 0.400000\n",
      "13\t0.2464\t-0.1250\t-0.0144\n",
      "\tSufficient decrease failed, reducing t 1.000000 --> 0.400000\n",
      "\tThe accepted step length is 0.400000\n",
      "14\t0.2522\t-0.1250\t0.0086\n",
      "\tSufficient decrease failed, reducing t 1.000000 --> 0.400000\n",
      "\tThe accepted step length is 0.400000\n",
      "15\t0.2487\t-0.1250\t-0.0052\n",
      "\tSufficient decrease failed, reducing t 1.000000 --> 0.400000\n",
      "\tThe accepted step length is 0.400000\n",
      "16\t0.2508\t-0.1250\t0.0031\n",
      "\tSufficient decrease failed, reducing t 1.000000 --> 0.400000\n",
      "\tThe accepted step length is 0.400000\n",
      "17\t0.2495\t-0.1250\t-0.0019\n",
      "\tSufficient decrease failed, reducing t 1.000000 --> 0.400000\n",
      "\tThe accepted step length is 0.400000\n",
      "18\t0.2503\t-0.1250\t0.0011\n",
      "\tSufficient decrease failed, reducing t 1.000000 --> 0.400000\n",
      "\tThe accepted step length is 0.400000\n",
      "\n",
      "Found the minimum near x* = 0.249832, true minimum is 0.25\n"
     ]
    }
   ],
   "source": [
    "# Set line search parameters\n",
    "\n",
    "beta1 = 0.4    # Step size multiplier if sufficient decrease fails\n",
    "beta2 = 1.2    # Step size multiplier if curvature condition fails\n",
    "alpha = 0.001  # Sufficient decrease parameter\n",
    "gamma = 0.5    # Curvature condition parameter\n",
    "\n",
    "\n",
    "# Set steepest descent parameters\n",
    "\n",
    "epsilon  = 0.001  # Stopping condition -- end when |df/dx| < epsilon \n",
    "max_iter = 100    # Maximum number of iterations\n",
    "it       = 0      # Current iteration\n",
    "\n",
    "\n",
    "# Initialize and iteratre\n",
    "\n",
    "x0   = 3      # Starting value of parameter\n",
    "x    = x0     # Current value of the parameter\n",
    "dfdx = df(x0) # Starting value of the derivative df/dx\n",
    "\n",
    "# Report status\n",
    "print('iter\\tx\\tf(x)\\tdf/dx')\n",
    "\n",
    "# Now loop through the steepest descent algorithm\n",
    "\n",
    "while np.fabs(dfdx)>=epsilon and it<max_iter:\n",
    "    \n",
    "    # Report status\n",
    "    print('%d\\t%.4f\\t%.4f\\t%.4f' % (it, x, f(x), dfdx))\n",
    "    \n",
    "    # Choose the step direction\n",
    "    s = -df(x) # FILL THIS IN\n",
    "    \n",
    "    # Choose how far to step in that direction\n",
    "    t = 1 \n",
    "    both_passed = False\n",
    "    \n",
    "    while not both_passed:\n",
    "\n",
    "        # Check for sufficient decrease fail\n",
    "\n",
    "        if f(x + (t*s)) > f(x) + (alpha * t * np.dot(df(x), s)):\n",
    "            print('\\tSufficient decrease failed, reducing t %lf --> %lf' % (t, beta1*t))\n",
    "            t = beta1 * t\n",
    "\n",
    "        # If passed, check for curvature condition fail\n",
    "\n",
    "        elif np.dot(df(x + (t*s)), s) < gamma * np.dot(df(x), s):\n",
    "            print('\\tCurvature condition failed, increasing t %lf --> %lf' % (t, beta2*t))\n",
    "            t = beta2 * t\n",
    "\n",
    "        # If both passed, exit the loop\n",
    "\n",
    "        else:\n",
    "            both_passed = True\n",
    "\n",
    "    print('\\tThe accepted step length is %lf' % t)\n",
    "    \n",
    "    # Update the parameters\n",
    "    x = x + t*s# FILL THIS IN\n",
    "    \n",
    "    # Update the derivative\n",
    "    dfdx = df(x) # FILL THIS IN\n",
    "    \n",
    "    # Update the iteration counter\n",
    "    it += 1\n",
    "    \n",
    "# Report the minimum\n",
    "print('\\nFound the minimum near x* = %lf, true minimum is 0.25' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "26WPdiTUPWJz"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3fDfYYzbPWJ5"
   },
   "source": [
    "### Possible issues, part 2\n",
    "\n",
    "This time we were able to converge to the minimum even though we started with the same initial conditions as in the previous lecture that resulted in an infinite loop. The line search algorithm led us to convergence. But steepest descent can still fail to converge in reasonable time.\n",
    "\n",
    "As an example, let's consider a **two-dimensional** quadratic function:\n",
    "\n",
    "$$\n",
    "f(\\underline{x}) = a_1 x_1^2 + a_2 x_2^2\\,.\n",
    "$$\n",
    "\n",
    "We'll choose $a_1 = 1$ and $a_2 = 1000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L3fBo-yRPWJ6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = np.array([1., 1000.])\n",
    "\n",
    "def f(x):\n",
    "    return np.sum(a * (x**2))\n",
    "\n",
    "def df(x):\n",
    "    return 2*a*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9RFo8xeEPWJ-"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QRV-bawxPWKB"
   },
   "source": [
    "Now what happens when we try to optimize with steepest descent? We know that the true minimum is at $(0, 0)$. Let's choose the starting point $\\underline{x}_0 = (0.9, 0.9)$ and try running the steepest descent algorithm below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1871
    },
    "colab_type": "code",
    "id": "VySbZehsPWKD",
    "outputId": "db502f20-b0fc-4a2c-b9ef-0bd905d02685",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter\tx1\tx2\tf(x)\tdf/dx1\tdf/dx2\n",
      "0\t0.900\t0.900\t810.810\t1.800\t1800.000\n",
      "1\t0.899\t-0.280\t79.011\t1.798\t-559.296\n",
      "2\t0.898\t0.087\t8.356\t1.795\t173.784\n",
      "3\t0.896\t-0.027\t1.533\t1.793\t-53.998\n",
      "4\t0.895\t0.008\t0.872\t1.791\t16.778\n",
      "5\t0.894\t-0.003\t0.806\t1.788\t-5.213\n",
      "6\t0.893\t0.001\t0.798\t1.786\t1.620\n",
      "7\t0.890\t-0.002\t0.796\t1.780\t-3.688\n",
      "8\t0.889\t0.001\t0.790\t1.778\t1.146\n",
      "9\t0.886\t-0.001\t0.787\t1.772\t-2.609\n",
      "10\t0.885\t0.000\t0.783\t1.770\t0.811\n",
      "11\t0.878\t-0.003\t0.779\t1.755\t-5.831\n",
      "12\t0.876\t0.001\t0.769\t1.753\t1.812\n",
      "13\t0.874\t-0.002\t0.767\t1.747\t-4.125\n",
      "14\t0.872\t0.001\t0.761\t1.745\t1.282\n",
      "15\t0.870\t-0.001\t0.758\t1.739\t-2.918\n",
      "16\t0.868\t0.000\t0.754\t1.737\t0.907\n",
      "17\t0.861\t-0.003\t0.752\t1.723\t-6.521\n",
      "18\t0.860\t0.001\t0.741\t1.720\t2.026\n",
      "19\t0.857\t-0.002\t0.740\t1.715\t-4.613\n",
      "20\t0.856\t0.001\t0.734\t1.712\t1.433\n",
      "21\t0.853\t-0.002\t0.731\t1.707\t-3.264\n",
      "22\t0.852\t0.001\t0.727\t1.705\t1.014\n",
      "23\t0.849\t-0.001\t0.723\t1.699\t-2.309\n",
      "24\t0.848\t0.000\t0.720\t1.697\t0.717\n",
      "25\t0.841\t-0.003\t0.715\t1.683\t-5.160\n",
      "26\t0.840\t0.001\t0.707\t1.681\t1.603\n",
      "27\t0.838\t-0.002\t0.705\t1.675\t-3.650\n",
      "28\t0.836\t0.001\t0.700\t1.673\t1.134\n",
      "29\t0.834\t-0.001\t0.697\t1.667\t-2.582\n",
      "30\t0.833\t0.000\t0.693\t1.665\t0.802\n",
      "31\t0.826\t-0.003\t0.690\t1.652\t-5.771\n",
      "32\t0.825\t0.001\t0.681\t1.649\t1.793\n",
      "33\t0.822\t-0.002\t0.680\t1.644\t-4.083\n",
      "34\t0.821\t0.001\t0.674\t1.642\t1.269\n",
      "35\t0.818\t-0.001\t0.672\t1.636\t-2.888\n",
      "36\t0.817\t0.000\t0.668\t1.634\t0.897\n",
      "37\t0.810\t-0.003\t0.667\t1.621\t-6.454\n",
      "38\t0.809\t0.001\t0.656\t1.619\t2.005\n",
      "39\t0.807\t-0.002\t0.656\t1.614\t-4.566\n",
      "40\t0.806\t0.001\t0.650\t1.611\t1.419\n",
      "41\t0.803\t-0.002\t0.648\t1.606\t-3.230\n",
      "42\t0.802\t0.001\t0.643\t1.604\t1.004\n",
      "43\t0.799\t-0.001\t0.640\t1.599\t-2.285\n",
      "44\t0.798\t0.000\t0.637\t1.597\t0.710\n",
      "45\t0.792\t-0.003\t0.633\t1.584\t-5.107\n",
      "46\t0.791\t0.001\t0.626\t1.582\t1.587\n",
      "47\t0.788\t-0.002\t0.624\t1.576\t-3.613\n",
      "48\t0.787\t0.001\t0.620\t1.574\t1.123\n",
      "49\t0.785\t-0.001\t0.617\t1.569\t-2.556\n",
      "50\t0.784\t0.000\t0.614\t1.567\t0.794\n",
      "51\t0.777\t-0.003\t0.612\t1.554\t-5.712\n",
      "52\t0.776\t0.001\t0.603\t1.552\t1.775\n",
      "53\t0.774\t-0.002\t0.602\t1.547\t-4.041\n",
      "54\t0.773\t0.001\t0.597\t1.545\t1.256\n",
      "55\t0.770\t-0.001\t0.595\t1.540\t-2.859\n",
      "56\t0.769\t0.000\t0.592\t1.538\t0.888\n",
      "57\t0.766\t-0.001\t0.588\t1.533\t-2.022\n",
      "58\t0.765\t0.000\t0.586\t1.531\t0.628\n",
      "59\t0.759\t-0.002\t0.581\t1.518\t-4.519\n",
      "60\t0.758\t0.001\t0.575\t1.516\t1.404\n",
      "61\t0.756\t-0.002\t0.574\t1.511\t-3.197\n",
      "62\t0.755\t0.000\t0.570\t1.509\t0.993\n",
      "63\t0.752\t-0.001\t0.567\t1.505\t-2.262\n",
      "64\t0.751\t0.000\t0.565\t1.503\t0.703\n",
      "65\t0.745\t-0.003\t0.562\t1.490\t-5.054\n",
      "66\t0.744\t0.001\t0.554\t1.488\t1.570\n",
      "67\t0.742\t-0.002\t0.553\t1.483\t-3.576\n",
      "68\t0.741\t0.001\t0.549\t1.481\t1.111\n",
      "69\t0.738\t-0.001\t0.547\t1.477\t-2.530\n",
      "70\t0.737\t0.000\t0.544\t1.475\t0.786\n",
      "71\t0.731\t-0.003\t0.543\t1.463\t-5.653\n",
      "72\t0.730\t0.001\t0.534\t1.461\t1.756\n",
      "73\t0.728\t-0.002\t0.534\t1.456\t-3.999\n",
      "74\t0.727\t0.001\t0.529\t1.454\t1.243\n",
      "75\t0.725\t-0.001\t0.527\t1.449\t-2.829\n",
      "76\t0.724\t0.000\t0.524\t1.447\t0.879\n",
      "77\t0.721\t-0.001\t0.521\t1.443\t-2.001\n",
      "78\t0.720\t0.000\t0.519\t1.441\t0.622\n",
      "79\t0.714\t-0.002\t0.515\t1.429\t-4.473\n",
      "80\t0.713\t0.001\t0.510\t1.427\t1.390\n",
      "81\t0.711\t-0.002\t0.508\t1.422\t-3.164\n",
      "82\t0.710\t0.000\t0.505\t1.420\t0.983\n",
      "83\t0.708\t-0.001\t0.502\t1.416\t-2.239\n",
      "84\t0.707\t0.000\t0.500\t1.414\t0.696\n",
      "85\t0.701\t-0.003\t0.498\t1.402\t-5.002\n",
      "86\t0.700\t0.001\t0.491\t1.401\t1.554\n",
      "87\t0.698\t-0.002\t0.490\t1.396\t-3.539\n",
      "88\t0.697\t0.001\t0.486\t1.394\t1.100\n",
      "89\t0.695\t-0.001\t0.484\t1.390\t-2.504\n",
      "90\t0.694\t0.000\t0.482\t1.388\t0.778\n",
      "91\t0.688\t-0.003\t0.481\t1.376\t-5.595\n",
      "92\t0.687\t0.001\t0.473\t1.375\t1.738\n",
      "93\t0.686\t-0.000\t0.471\t1.373\t-0.540\n",
      "94\t0.681\t0.002\t0.467\t1.361\t3.885\n",
      "95\t0.680\t-0.001\t0.463\t1.360\t-1.207\n",
      "96\t0.678\t0.001\t0.461\t1.355\t2.748\n",
      "97\t0.677\t-0.000\t0.458\t1.353\t-0.854\n",
      "98\t0.675\t0.001\t0.456\t1.349\t1.944\n",
      "99\t0.674\t-0.000\t0.454\t1.347\t-0.604\n",
      "\n",
      "Found the minimum near x* = (0.668118, 0.002172), true minimum is (0, 0)\n"
     ]
    }
   ],
   "source": [
    "# Set line search parameters\n",
    "\n",
    "beta1 = 0.4    # Step size multiplier if sufficient decrease fails\n",
    "beta2 = 1.2    # Step size multiplier if curvature condition fails\n",
    "alpha = 0.001  # Sufficient decrease parameter\n",
    "gamma = 0.5    # Curvature condition parameter\n",
    "\n",
    "\n",
    "# Set steepest descent parameters\n",
    "\n",
    "epsilon  = 0.001  # Stopping condition -- end when |df/dx| < epsilon \n",
    "max_iter = 100    # Maximum number of iterations\n",
    "it       = 0      # Current iteration\n",
    "\n",
    "\n",
    "# Initialize and iteratre\n",
    "\n",
    "x0   = np.array([0.9, 0.9]) # Starting value of parameter\n",
    "x    = x0                   # Current value of the parameter\n",
    "dfdx = df(x0)               # Starting value of the derivative df/dx\n",
    "\n",
    "# Report status\n",
    "print('iter\\tx1\\tx2\\tf(x)\\tdf/dx1\\tdf/dx2')\n",
    "\n",
    "# Now loop through the steepest descent algorithm\n",
    "\n",
    "while np.sum(np.fabs(dfdx))>=epsilon and it<max_iter:\n",
    "    \n",
    "    # Report status\n",
    "    print('%d\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f' % (it, x[0], x[1], f(x), dfdx[0], dfdx[1]))\n",
    "    \n",
    "    # Choose the step direction\n",
    "    s = -df(x)\n",
    "    \n",
    "    # Choose how far to step in that direction\n",
    "    t = 1 \n",
    "    both_passed = False\n",
    "    \n",
    "    while not both_passed:\n",
    "\n",
    "        # Check for sufficient decrease fail\n",
    "\n",
    "        if f(x + (t*s)) > f(x) + (alpha * t * np.dot(df(x), s)):\n",
    "            #print('\\tSufficient decrease failed, reducing t %lf --> %lf' % (t, beta1*t))\n",
    "            t = beta1 * t\n",
    "\n",
    "        # If passed, check for curvature condition fail\n",
    "\n",
    "        elif np.dot(df(x + (t*s)), s) < gamma * np.dot(df(x), s):\n",
    "            #print('\\tCurvature condition failed, increasing t %lf --> %lf' % (t, beta2*t))\n",
    "            t = beta2 * t\n",
    "\n",
    "        # If both passed, exit the loop\n",
    "\n",
    "        else:\n",
    "            both_passed = True\n",
    "\n",
    "    #print('\\tThe accepted step length is %lf' % t)\n",
    "    \n",
    "    # Update the parameters\n",
    "    x = x + t*s\n",
    "    \n",
    "    # Update the derivative\n",
    "    dfdx = df(x)\n",
    "    \n",
    "    # Update the iteration counter\n",
    "    it += 1\n",
    "    \n",
    "# Report the minimum\n",
    "print('\\nFound the minimum near x* = (%lf, %lf), true minimum is (0, 0)' % (x[0], x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iqWqpl70PWKJ"
   },
   "source": [
    "Again, we've fallen into a trap! This is due to the vast difference in **curvature** between these two directions. Next time we will discuss another method to avoid this kind of issue."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "lecture-16.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
